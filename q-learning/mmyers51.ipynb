{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Programming Assignment\n",
    "\n",
    "## Directions\n",
    "\n",
    "There are general instructions on Blackboard and in the Syllabus for Programming Assignments. This Notebook also has instructions specific to this assignment. Read all the instructions carefully and make sure you understand them. Please ask questions on the discussion boards or email me at `EN605.445@gmail.com` if you do not understand something.\n",
    "\n",
    "<div style=\"background: mistyrose; color: firebrick; border: 2px solid darkred; padding: 5px; margin: 10px;\">\n",
    "You must follow the directions *exactly* or you will get a 0 on the assignment.\n",
    "</div>\n",
    "\n",
    "You must submit a zip file of your assignment and associated files (if there are any) to Blackboard. The zip file will be named after you JHED ID: `<jhed_id>.zip`. It will not include any other information. Inside this zip file should be the following directory structure:\n",
    "\n",
    "```\n",
    "<jhed_id>\n",
    "    |\n",
    "    +--module-04-programming.ipynb\n",
    "    +--module-04-programming.html\n",
    "    +--world.txt\n",
    "    +--test01.txt\n",
    "    +--(any other files)\n",
    "```\n",
    "\n",
    "For example, do not name  your directory `programming_assignment_01` and do not name your directory `smith122_pr1` or any else. It must be only your JHED ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import *\n",
    "from StringIO import StringIO\n",
    "\n",
    "# add whatever else you need from the Anaconda packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning with Q-Learning\n",
    "\n",
    "This problem is very similar to the one in Module 1 that we solved with A\\* search but this time we're going to use a different approach.\n",
    "\n",
    "We're replacing the deterministic movement from that module with stochastic movement. This means that actions don't result in a fixed successor state but a probability distribution over successor states and the successor state we want may not be the successor state we get. \n",
    "\n",
    "There are a variety of ways to handle this problem. If the agent finds itself off the solution, you can simply calculate a new solution from where the agent ended up. Although this sounds like a really bad idea, it has actually been shown to work really well in Video Games that use formal Planning algorithms (which we will cover later).\n",
    "\n",
    "Another approach is to use Reinforcement Learning. There are a variety of options there: model-based and model-free, Value Iteration, Q-Learning and SARSA. You are going to use Q-Learning.\n",
    "\n",
    "## The World Representation\n",
    "\n",
    "As before, we're going to simplify the problem by working in a grid world. The symbols that form the grid have a special meaning as they specify the type of the terrain and the cost to enter a grid cell with that type of terrain:\n",
    "\n",
    "```\n",
    "token   terrain    cost \n",
    ".       plains     1\n",
    "*       forest     3\n",
    "^       hills      5\n",
    "~       swamp      7\n",
    "x       mountains  impassible\n",
    "```\n",
    "\n",
    "When you go from a plains node to a forest node it costs 3. When you go from a forest node to a plains node, it costs 1. You can think of the grid as a big graph. Each grid cell (terrain symbol) is a node and there are edges to the north, south, east and west (except at the edges).\n",
    "\n",
    "There are quite a few differences between A\\* Search and Reinforcement Learning but one of the most salient is that A\\* Search returns a plan of N steps that gets us from A to Z, for example, A->C->E->G.... Reinforcement Learning, on the other hand, returns  a *policy* that tells us what the best thing to do for every and any state. \n",
    "\n",
    "For example, the policy might say that the best thing to do in A is go to C. However, we might find ourselves in D. In which case, the policy might say, D->E. Trying this action might land us in C and the policy will say, C->E, etc. At least with offline learning, everything will be learned in advance (in online learning, you can only learn by doing and so you may act according to a known but suboptimal policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Map\n",
    "\n",
    "To avoid global variables, we have a <code>read_world()</code> function that takes a filename and returns the world as `List` of `List`s. **The same coordinates reversal applies: (x, y) is world[ y][ x] as from PR01.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_world( filename):\n",
    "    with open( filename, 'r') as f:\n",
    "        world_data = [x for x in f.readlines()]\n",
    "    f.closed\n",
    "    world = []\n",
    "    for line in world_data:\n",
    "        line = line.strip()\n",
    "        if line == \"\": continue\n",
    "        world.append([x for x in line])\n",
    "    return world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a dict of movement costs. Note that we've negated them this time because RL requires negative costs and positive rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "costs = { '.': -1, '*': -3, '^': -5, '~': -7}\n",
    "costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a list of offsets for `cardinal_moves`. You'll need to work this into your actions, A, parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cardinal_moves = [(0,-1), (1,0), (0,1), (-1,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the confusing bits begin. We must program both the Q-Learning algorithm and a *simulator*. The Q-Learning algorithm doesn't know T but the simulator *must*. Essentially the *simulator* is any time you apply a move and check to see what state you actually end up in (rather than the state you planned to end up in).\n",
    "\n",
    "The transition function your *simulation* should use, T, is 0.70 for the desired direction, and 0.10 each for the other possible directions. That is, if I select \"up\" then 70% of the time, I go up but 10% of the time I go left, 10% of the time I go right and 10% of the time I go down. If you're at the edge of the map, you simply bounce back to the current state.\n",
    "\n",
    "You need to implement `q_learning()` with the following parameters:\n",
    "\n",
    "+ world: a `List` of `List`s of terrain (this is S from S, A, T, gamma, R)\n",
    "+ costs: a `Dict` of costs by terrain (this is part of R)\n",
    "+ goal: A `Tuple` of (x, y) stating the goal state.\n",
    "+ reward: The reward for achieving the goal state.\n",
    "+ actions: a `List` of possible actions, A, as offsets.\n",
    "+ gamma: the discount rate\n",
    "+ alpha: the learning rate\n",
    "\n",
    "you will return a policy: \n",
    "\n",
    "`{(x1, y1): action1, (x2, y2): action2, ...}`\n",
    "\n",
    "Remember...a policy is what to do in any state for all the states. Notice how this is different that A\\* search which only returns actions to take from the start to the goal. This also explains why `q_learning` doesn't take a `start` state!\n",
    "\n",
    "You should also define a function `pretty_print_policy( cols, rows, policy)` that takes a policy and prints it out as a grid using \"^\" for up, \"<\" for left, \"v\" for down and \">\" for right. Note that it doesn't need the `world` because the policy has a move for every state. However, you do need to know how big the grid is so you can pull the values out of the `Dict` that is returned.\n",
    "\n",
    "```\n",
    "vvvvvvv\n",
    "vvvvvvv\n",
    "vvvvvvv\n",
    ">>>>>>v\n",
    "^^^>>>v\n",
    "^^^>>>v\n",
    "^^^>>>G\n",
    "```\n",
    "\n",
    "(Note that that policy is completely made up and only illustrative of the desired output).\n",
    "\n",
    "There are a lot of details that I have left up to you. For example, when do you stop? Is there a strategy for learning the policy? Watch and re-watch the lecture on Q-Learning. Ask questions. You need to implement a way to pick initial states for each iteration and you need a way to balance exploration and exploitation while learning. You may have to experiment with different gamma and alpha values. Be careful with your reward...the best reward is related to the discount rate and the approxmiate number of actions you need to reach the goal.\n",
    "\n",
    "* If everything is otherwise the same, do you think that the path from (0,0) to the goal would be the same for both A\\* Search and Q-Learning?\n",
    "* What do you think if you have a map that looks like:\n",
    "\n",
    "```\n",
    "><>>^\n",
    ">>>>v\n",
    ">>>>v\n",
    ">>>>v\n",
    ">>>>G\n",
    "```\n",
    "\n",
    "has this converged?\n",
    "\n",
    "\n",
    "**Work with a smaller test world to start!** Name it `test01.txt` and return it with your assignment. I suggest it be asymmetric to be avoid corner cases (5x6 for example).\n",
    "\n",
    "Remember that you should follow the general style guidelines for this course: well-named, short, focused functions with limited indentation using Markdown documentation that explains their implementation and the AI concepts behind them.\n",
    "\n",
    "This assignment sometimes wrecks havoc with IPython notebook, save often. Put your helper functions here, along with their documentation. There should be one Markdown cell for the documentation, followed by one Codecell for the implementation.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Pick initial state**\n",
    "\n",
    "In the q_learning algorithm, an initial state must be selected for each episode.  pick_initial_state selects a random state in the world at which the agent will start its exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_initial_state(world):\n",
    "    y_coordinate = random.randint(0, len(world) - 1)\n",
    "    \n",
    "    x_coordinate = random.randint(0, len(world[0]) - 1)\n",
    "\n",
    "    return (x_coordinate, y_coordinate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "&nbsp;\n",
    "\n",
    "**Initialize zero array**\n",
    "\n",
    "In the q_learning algorithm, Q must be initialized to all zeros.  initialize_zero_array uses the dimensions of the given world to create an two dimensional array of the same size initialized with all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_zero_array(world):\n",
    "    zero_array = []\n",
    "    height = len(world)\n",
    "    width = len(world[0])\n",
    "    zeros = [0] * width\n",
    "\n",
    "    for i in range(height):\n",
    "        new_zero_array = copy.deepcopy(zeros)\n",
    "        zero_array.append(new_zero_array)\n",
    "\n",
    "    return zero_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "&nbsp;\n",
    "\n",
    "**Initialize zeros**\n",
    "\n",
    "initialize_zeros utilizes initialize_zero_array to create a dictionary where each action is the key, and the value is a multidimensional array of zeros.  This format of dictionary is used to keep track of q values (variable 'q') as well as a count of which state action pairs have been visited (variable 'visits').  The format is { action1: [[0,0...], [0,0...]...], action2: [[0,0...], [0,0...]...] }.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_zeros(world, actions):\n",
    "    q = {}\n",
    "    for action in actions:\n",
    "        q[action] = initialize_zero_array(world)\n",
    "\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "&nbsp;\n",
    "\n",
    "**Is valid**\n",
    "\n",
    "Given a state, an action, and the world, is_valid determines whether that action is valid for that state.  The action is valid only if taking it would not cause the state to be outside of the world or land on impassible terrain.  This function is used to determine which actions to take from a state, and to get the final policy using the q values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_valid(state, action, world):\n",
    "    x_coordinate = state[0] + action[0]\n",
    "    y_coordinate = state[1] + action[1]\n",
    "\n",
    "    world_depth = len(world)\n",
    "    if y_coordinate < 0 or y_coordinate >= world_depth:\n",
    "        return False\n",
    "\n",
    "    world_width = len(world[0])\n",
    "    if x_coordinate < 0 or x_coordinate >= world_width:\n",
    "        return False\n",
    "\n",
    "    if world[y_coordinate][x_coordinate] == 'x':\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "&nbsp;\n",
    "\n",
    "**Get actions**\n",
    "\n",
    "get_actions determines the best action to take from a given state as well as the other available actions.  For each possible action, it checks if that action is valid for the state using is_valid.  Out of the actions that are valid, it selects the action that has been visited the least using the visits data structure.  It returns two values: selected_action, and an array other_actions.  The values returned are used by the simulator when selecting an action to execute based on the percentages defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_actions(state, visits, actions, world):\n",
    "    min_visits = float(\"inf\")\n",
    "    selected_action = None\n",
    "    other_actions = []\n",
    "    for action in actions:\n",
    "        if is_valid(state, action, world):\n",
    "            x_coordinate = state[0]\n",
    "            y_coordinate = state[1]\n",
    "            num_visits = visits[action][y_coordinate][x_coordinate]\n",
    "            if num_visits < min_visits:\n",
    "                min_visits = num_visits\n",
    "\n",
    "                if selected_action and selected_action not in other_actions:\n",
    "                    other_actions.append(selected_action)\n",
    "\n",
    "                selected_action = action\n",
    "\n",
    "            elif action not in other_actions:\n",
    "                other_actions.append(action)\n",
    "\n",
    "    if selected_action in other_actions:\n",
    "        other_actions.remove(selected_action)\n",
    "\n",
    "    return selected_action, other_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "&nbsp;\n",
    "\n",
    "**Get max action**\n",
    "\n",
    "get_max_action_value is used for the q value calculation.  The formula for Q[s, a] requires finding maxa(Q[s', a']), and this function finds that value.  Given a state, actions, and the q data structure, it returns the maximum q value for a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_max_action_value(state, actions, q):\n",
    "    max = float(\"-inf\")\n",
    "\n",
    "    for action in actions:\n",
    "        x_coordinate = state[0]\n",
    "        y_coordinate = state[1]\n",
    "        q_value = q[action][y_coordinate][x_coordinate]\n",
    "        if q_value > max:\n",
    "            max = q_value\n",
    "\n",
    "    return max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "&nbsp;\n",
    "\n",
    "**Calculate Q value**\n",
    "\n",
    "calculate_q_value calculates the Q[s, a] value required for the q_learning algorithm.  Given alpha, gamma, reward, q, action, state, actions, new_state it returns the value for the formula Q[s, a] = (1 - alpha) * q_value + alpha * (reward + gamma * max_action_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_q_value(alpha, gamma, reward, q, action, state, actions, new_state):\n",
    "    max_action_value = get_max_action_value(new_state, actions, q)\n",
    "\n",
    "    x_coordinate = state[0]\n",
    "    y_coordinate = state[1]\n",
    "    q_value = q[action][y_coordinate][x_coordinate]\n",
    "\n",
    "    new_q_value = (1 - alpha) * q_value + alpha * (reward + gamma * max_action_value)\n",
    "\n",
    "    return new_q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "&nbsp;\n",
    "\n",
    "**Check for convergence**\n",
    "\n",
    "check_for_convergence determines whether the q_learning algorithm has converged.  It compares the values in the current q structure with the values of the q structure in the previous episode.  If the absolute value difference for every value comparison is less then epsilon, it returns true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_for_convergence(q, previous_q, actions):\n",
    "    epsilon = 0.2\n",
    "\n",
    "    for action in actions:\n",
    "        q_values = q[action]\n",
    "        previous_q_values = previous_q[action]\n",
    "        for i in range(len(q_values)):\n",
    "            row = q_values[i]\n",
    "            for j in range(len(row)):\n",
    "                if abs(q_values[i][j] - previous_q_values[i][j]) > epsilon:\n",
    "                    return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def q_learning( world, costs, goal, reward, actions, gamma, alpha):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretty_print_policy( rows, cols, policy):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_world = read_world( \"test01.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# goal = ?? # FILL ME IN\n",
    "# gamma = ?? # FILL ME IN\n",
    "# alpha = ?? # FILL ME IN\n",
    "\n",
    "test_policy = q_learning( test_world, costs, goal, reward, cardinal_moves, gamma, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cols = ?? # FILL ME IN\n",
    "# rows = ?? # FILL ME IN\n",
    "\n",
    "pretty_print_policy( cols, rows, test_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_world = read_world( \"world.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# goal = ?? # FILL ME IN\n",
    "# gamma = ?? # FILL ME IN\n",
    "# alpha = ?? # FILL ME IN\n",
    "\n",
    "full_policy = q_learning( full_world, costs, goal, reward, cardinal_moves, gamma, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cols = ?? # FILL ME IN\n",
    "# rows = ?? # FILL ME IN\n",
    "\n",
    "pretty_print_policy( cols, rows, full_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
